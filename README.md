# Local Coding Agent with Ollama

A **local-only** coding agent that uses Ollama through its OpenAI-compatible API with tool calling capabilities. Works entirely offline with open-weight models.

## ğŸ¯ Why Local-Only Coding Agents Matter

### The Problem with Cloud-Based AI Coding Assistants

While cloud-based AI coding assistants have revolutionized software development, they come with significant limitations:

**ğŸ”’ Privacy & Security Concerns**
- Your proprietary code is sent to third-party servers
- Enterprise codebases risk intellectual property exposure  
- Compliance issues with regulations like GDPR, HIPAA, SOX
- No guarantee your code won't be used for training future models

**ğŸ’¸ Cost Escalation**
- Usage-based pricing can become expensive for heavy users
- Team licenses multiply costs across organizations
- API rate limits can interrupt workflows
- Unpredictable monthly bills as usage scales

**ğŸ“¡ Internet Dependency**
- Requires stable internet connection to function
- Latency affects response times and user experience
- Complete failure during network outages
- Unreliable in remote locations or poor connectivity areas

**ğŸ›ï¸ Limited Customization**
- Locked into vendor's model choices and capabilities
- Cannot fine-tune models for your specific domain
- Limited control over system prompts and behavior
- Vendor lock-in with proprietary APIs and workflows

### The Local-Only Solution

This project addresses these challenges by bringing AI coding assistance directly to your machine:

**ğŸ  Complete Privacy**
- Your code never leaves your machine
- Zero data transmission to external servers
- Full compliance with enterprise security policies
- Perfect for sensitive or classified projects

**ğŸ’° Zero Ongoing Costs**
- One-time setup with no recurring fees
- Run unlimited queries without usage billing
- Scale across teams without per-seat licensing
- Predictable infrastructure costs

**âš¡ Always Available**
- Works completely offline after initial setup
- No network latency - instant responses
- Reliable operation regardless of internet status
- Perfect for air-gapped environments

**ğŸ¨ Full Customization**
- Choose from multiple open-weight models
- Fine-tune models for your specific use cases  
- Customize system prompts and behavior
- Add domain-specific tools and capabilities

**ğŸ”§ Tool Calling & Function Integration**
- Native function calling with local file system access
- Execute commands safely within sandboxed workspace
- Integrate with your existing development tools
- Extensible architecture for custom tool development

### Perfect for These Use Cases

- **Enterprise Development**: Companies with strict security requirements
- **Remote/Offline Work**: Developers in areas with poor connectivity
- **Educational Institutions**: Teaching environments without internet access
- **Government/Defense**: Air-gapped systems and classified projects
- **Cost-Conscious Teams**: Startups and small teams avoiding subscription fees
- **Privacy-First Developers**: Those who prioritize data sovereignty
- **Research & Experimentation**: Academic research requiring reproducibility

### Performance Comparison

| Feature | Cloud Agents | Local Agent |
|---------|-------------|-------------|
| Privacy | âŒ Code sent externally | âœ… Fully local |
| Cost | âŒ Monthly subscriptions | âœ… One-time setup |
| Offline | âŒ Internet required | âœ… Works offline |
| Latency | âš ï¸ Network dependent | âœ… Instant response |
| Customization | âš ï¸ Limited options | âœ… Full control |
| Enterprise | âš ï¸ Compliance issues | âœ… Audit-friendly |

This local coding agent proves that you don't need to sacrifice privacy, pay ongoing costs, or depend on internet connectivity to get powerful AI coding assistance.

## Setup

1. **Install Ollama** (if not already installed):
   ```bash
   # macOS
   brew install --cask ollama
   
   # Linux
   curl -fsSL https://ollama.com/install.sh | sh
   ```

2. **Start Ollama daemon**:
   ```bash
   ollama serve >/tmp/ollama.log 2>&1 &
   ```

3. **Pull required models**:
   ```bash
   ollama pull llama3.1:8b          # Main agent model
   ollama pull deepseek-r1:7b       # Oracle/reasoning model
   ollama pull deepseek-coder-v2    # Alternative coding model
   ```

4. **Set up Python environment**:
   ```bash
   python3 -m venv .venv && source .venv/bin/activate
   pip install -r requirements.txt
   ```

5. **Load environment**:
   ```bash
   set -a; source .env; set +a
   ```

## Usage

### ğŸ¨ TUI Mode (Recommended)
Beautiful terminal interface with dark theme and organized panels:
```bash
source .venv/bin/activate
python agent_ollama.py --tui
```

**TUI Features:**
- ğŸ“ **File Explorer**: Browse and manage workspace files
- ğŸ’¬ **Chat Interface**: Interactive conversation with the agent
- ğŸ“Š **Status Panel**: Real-time agent status and statistics
- ğŸ”§ **Tool Outputs**: Monitor tool execution and results
- âŒ¨ï¸ **Keyboard Shortcuts**: Efficient navigation and controls

### ğŸ’» Interactive CLI Mode
Traditional command-line interface:
```bash
source .venv/bin/activate
python agent_ollama.py
```

### âš¡ Single Command Mode
Execute one-off commands:
```bash
python agent_ollama.py "create a simple Python hello world script"
```

### ğŸ”® Oracle Mode (reasoning assistant)
In interactive mode, use `/oracle <question>` to consult the reasoning model:
```
/oracle How should I structure a REST API in Python?
```

## Available Tools

The agent has access to these tools:
- **read_file**: Read file contents
- **write_file**: Write content to files
- **list_directory**: List files and directories
- **run_command**: Execute shell commands
- **search_files**: Search for text patterns in files

## Configuration

Edit `.env` to customize:
- `AGENT_MODEL`: Main model for tool calling (default: llama3.1:8b)
- `ORACLE_MODEL`: Reasoning model (default: deepseek-r1:7b)
- `WORKSPACE`: Working directory (default: ./ws)

## Features

- ğŸ  **Fully local**: No internet required after setup
- ğŸ”§ **Tool calling**: Uses function calling for file operations
- ğŸ§  **Reasoning**: Oracle model for code review and planning
- ğŸ”’ **Sandboxed**: Works within designated workspace
- âš¡ **Fast**: Direct Ollama API calls
- ğŸ¯ **Context-aware**: Maintains conversation context

## Examples

```bash
# Create a Python project
python agent_ollama.py "create a simple web server using Flask"

# Debug code
python agent_ollama.py "find and fix any syntax errors in the Python files"

# Code review
python agent_ollama.py "review the code quality and suggest improvements"
```
